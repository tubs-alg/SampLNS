{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Cleaning the data and providing a small and easily parsable database.\n",
    "\n",
    "The database with the full logs is too huge to work on. Most of the data would only be needed for debugging but not for evaluation.\n",
    "Thus, we create a clean and small database for the further evaluation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the data folders: {'2023-03-01_13-51-03/'}\n",
      "Removed 0 results because of size constraint.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the data of the experiment.\n",
    "from _utils import get_results, parse_sample\n",
    "\n",
    "# CHECK THAT THESE ARE THE CORRECT PATHS!\n",
    "INPUT_SAMPLE_ARCHIVE = \"./00_baseline/900_seconds_5_it.zip\"\n",
    "INSTANCE_ARCHIVE = \"./00_benchmark_instances.zip\"\n",
    "RESULT_FOLDER = \"./01_results/900_seconds_5_it_900/\"\n",
    "\n",
    "\n",
    "def recache_data():\n",
    "    relevant_columns = [\"instance\", \"#Variables\", \"#Clauses\", \"iteration_info\",\n",
    "                        \"baseline_alg\",\n",
    "                        \"initial_sample_path\", \"SampleSize\", \"lower_bound\", \"upper_bound\"]\n",
    "    data = get_results(input_sample_archive=INPUT_SAMPLE_ARCHIVE,\n",
    "                       result_folder=RESULT_FOLDER)[relevant_columns]\n",
    "    data.rename(columns={\"SampleSize\": \"initial_sample_size\", \"upper_bound\": \"optimized_sample_size\"}, inplace=True)\n",
    "    data.drop_duplicates(subset=[\"initial_sample_path\"], inplace=True)\n",
    "    data.to_json(\"./05_clean_data.json.zip\")\n",
    "    del data\n",
    "\n",
    "recache_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['instance', '#Variables', '#Clauses', 'iteration_info', 'baseline_alg',\n       'initial_sample_path', 'initial_sample_size', 'lower_bound',\n       'optimized_sample_size'],\n      dtype='object')"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_json(\"./05_clean_data.json.zip\")\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* `instance` Unique name of feature model.\n",
    "* `#Variables` Number of variables in model.\n",
    "* `#Clauses` Number of clauses in model.\n",
    "* `iteration_info` Information on the individual iterations (needs some additional processing)\n",
    "* `baseline_alg` Name of the algorithm that computed the initial sample.\n",
    "* `initial_sample_path` Path to the initial sample in the database (for querying, but it also serves as identifier).\n",
    "* `initial_sample_size` Size of the initial sample.\n",
    "* `lower_bound` The lower bound computed by SampLNS.\n",
    "* `optimized_sample_size` Size of the sample after optimization with SampLNS."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
